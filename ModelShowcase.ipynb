{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Demo Showcase\n",
    " this notebook is part of the showcase for the mid semester checkpoint.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:04:03.062770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 13:04:05.418740: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/andyd/miniconda3/envs/nlp/lib/\n",
      "2023-03-16 13:04:05.422198: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/andyd/miniconda3/envs/nlp/lib/\n",
      "2023-03-16 13:04:05.422246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# all imports (collapsed)\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from time import strftime, localtime\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ARTICLE_LENGTH = 10000\n",
    "PAD_VALUE = 0\n",
    "\n",
    "db = sqlite3.connect('text_encoder.db')\n",
    "c = db.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS \"TextEncoder\"(\n",
    "    \"word\" TEXT,\n",
    "    \"n\" INTEGER,\n",
    "    PRIMARY KEY(\"word\")\n",
    ");''')\n",
    "del c\n",
    "\n",
    "def clean_text(raw_text, to_lower=True):\n",
    "    text = re.sub(r'(https|http):[^\\s]*', ' ', raw_text)\n",
    "    text = re.sub('[' + re.escape('#@!\"$%&(\\')*+,-./:;<=>?[\\\\]^_`{|}~') + ']', '', text)\n",
    "    text = text.encode('utf8').decode('ascii','ignore')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text.lower()\n",
    "\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, features='lxml')\n",
    "    p_tags = soup.find_all('p')\n",
    "    raw_text = ' '.join([p.text for p in p_tags])\n",
    "    text = clean_text(raw_text)\n",
    "    return text\n",
    "\n",
    "def get_max_key():\n",
    "    c = db.cursor()\n",
    "    c.execute('SELECT MAX(n) FROM TextEncoder;')\n",
    "    return c.fetchone()[0]\n",
    "\n",
    "def add_word_encoding(word, key):\n",
    "    c = db.cursor()\n",
    "    c.execute('INSERT INTO TextEncoder (word, n) VALUES (?, ?);', (word, key))\n",
    "    assert c.rowcount > 0, \"ERROR Inserting key %10s as word: %20s\" % (key, word)\n",
    "    db.commit()\n",
    "    return\n",
    "\n",
    "\n",
    "def get_word_key(word, freeze):\n",
    "    c = db.cursor()\n",
    "    c.execute('SELECT n FROM TextEncoder WHERE word = ?;', (word,))\n",
    "    response = c.fetchone()\n",
    "    if response is not None:\n",
    "        return response[0]\n",
    "    else:\n",
    "        if freeze:\n",
    "            return None\n",
    "        else:\n",
    "            key = get_max_key() + 1\n",
    "            add_word_encoding(word, key)\n",
    "            print('Adding word \"%20s\" as n: %10i' % (word, key))\n",
    "            return key\n",
    "\n",
    "\n",
    "def numberize_article(article, freeze):\n",
    "    tokens = text_to_word_sequence(article)\n",
    "    num_article = []\n",
    "    for token in tokens:\n",
    "        key = get_word_key(token, freeze)\n",
    "        if key is not None:\n",
    "            num_article.append(key)\n",
    "    return np.array(num_article)\n",
    "\n",
    "\n",
    "def parse_articles(articles, max_length=10000, min_length=20, verbose=False, freeze=False):\n",
    "    numerified_articles = []\n",
    "    removed_articles = set()\n",
    "    for i, a in enumerate(articles):\n",
    "        numberized = numberize_article(a, freeze)\n",
    "        if len(numberized) >= min_length:\n",
    "            numerified_articles.append(numberized)\n",
    "        else:\n",
    "            removed_articles.add(i)\n",
    "            if verbose:\n",
    "                print('Removing article: %6i for being too short. Length: %3i' % (i, len(numberized)))\n",
    "    numerified_articles = np.array(numerified_articles)\n",
    "    return pad_sequences(numerified_articles, maxlen=max_length, dtype='float32', value=0), removed_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:04:07.818140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:07.931066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:07.931130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:07.933232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 13:04:07.937390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:07.937460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:07.937491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:09.539112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:09.539721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:09.539738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-16 13:04:09.539812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-16 13:04:09.540376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5909 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = load_model('savedModels/article_lstm_model_230316.115129.h5') #output hidden to remove clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10000, 100)        17328600  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10000, 100)        0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10000, 32)         9632      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 5000, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5000, 32)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,391,533\n",
      "Trainable params: 17,391,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text, model=model):\n",
    "    parsed_article, _ = parse_articles([text], min_length=0, freeze=True)\n",
    "    if len(parsed_article) < 1:\n",
    "        print('Sorry! It looks like once I processed that article, it was too short to use. :\\'(')\n",
    "        return\n",
    "    print('ðŸ¤” Let me thinkâ€¦')\n",
    "    prediction = model.predict(parsed_article)\n",
    "    prediction_pct = prediction[0] * 100\n",
    "    print('''\\n   ------------------------------------------------------------------------------------------\\n  |   I\\'d say that there\\'s a %5.2f%% chance that your article is from a right-leaning source. |\\n  |  ----------------------------------------------------------------------------------------\\n  |/\\nðŸ–¥ï¸\\n''' % prediction_pct)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤” Let me thinkâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:04:12.763510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "   ------------------------------------------------------------------------------------------\n",
      "  |   I'd say that there's a 99.71% chance that your article is from a right-leaning source. |\n",
      "  |  ----------------------------------------------------------------------------------------\n",
      "  |/\n",
      "ðŸ–¥ï¸\n",
      "\n",
      "ðŸ¤” Let me thinkâ€¦\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "\n",
      "   ------------------------------------------------------------------------------------------\n",
      "  |   I'd say that there's a 76.57% chance that your article is from a right-leaning source. |\n",
      "  |  ----------------------------------------------------------------------------------------\n",
      "  |/\n",
      "ðŸ–¥ï¸\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze articles from a set test script\n",
    "test_urls = ['https://www.washingtonpost.com/politics/2023/03/15/trump-fox-news-dominion-defamation-lawsuit/',\n",
    "        'https://www.washingtonpost.com/politics/2023/03/15/biden-medicare-prescription-drugs-fight/']\n",
    "for test in test_urls: make_prediction(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ðŸ¤” Let me thinkâ€¦\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "\n",
      "   ------------------------------------------------------------------------------------------\n",
      "  |   I'd say that there's a 60.40% chance that your article is from a right-leaning source. |\n",
      "  |  ----------------------------------------------------------------------------------------\n",
      "  |/\n",
      "ðŸ–¥ï¸\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze articles from any url input\n",
    "url = input('Enter a url: ').strip()\n",
    "print('\\n\\n')\n",
    "make_prediction(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
